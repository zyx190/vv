---
layout: post
title:
categories:
- Python
- 机器学习
tags:
- Python
- 机器学习
- 线性模型
typora-root-url: ./..
---

## 基本形式

对样本$x=(x_1;x_2;x_3;...;x_d)$，有

$$
f(x_i)=\omega^Tx+b
$$

其中$\omega=(\omega_1;\omega_2;...;\omega_d)$

## 线性回归

离散属性处理

-   具有序关系的属性可以连续化，如身高
-   不具有序关系的属性转化为k维向量

### 一元线性回归

对于一元线性回归
$$
f(x_i)=\omega x_i+b
$$

使用均方误差来衡量$f(x)$与$y$​的差别，并使其最小化来求解模型的方法称为最小二乘法

最小二乘参数估计

$$
E(\omega,b)=\sum\limits_{i=1}^m(f(x_i)-y_i)^2\\
\frac{\part E(\omega,b)}{\part \omega}=2(\omega\sum\limits_{i=1}^mx_i^2-\sum\limits_{i=1}^m(y_i-b)x_i)\\
\frac{\part E(\omega,b)}{\part b}=2(mb-\sum\limits_{i=1}^m(y_i-\omega x_i))
$$

令两个偏导为0可求得$\omega$和$b$最优解

### 多元线性回归

对于多元线性回归

$$
f(x_i)=\omega^T x_i+b
$$

定义向量形式如下

$$
\hat{\omega}=(\vec{\omega},b)\\
X=\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1\\
x_{21} & x_{22} & \cdots & x_{2d} & 1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
x_{m1} & x_{m2} & \cdots & x_{md} & 1\\
\end{bmatrix}=\begin{bmatrix}
\vec{x}_1^T & 1\\
\vec{x}_2^T & 1\\
\vdots & \vdots\\
\vec{x}_m^T & 1\\
\end{bmatrix}\\
\vec{y}=(y_1;y_2;...;y_m)
$$

由最小二乘法有

$$
E(\hat{\omega})=(\vec{y}-X\hat{\omega})^T(\vec{y}-X\hat{\omega})\\
\frac{\part E(\hat{\omega})}{\part \omega}=2X^T(X\hat{\omega}-\vec{y})
$$

当$X^TX$满秩时，令偏导为0有唯一解，也称为正规方程法

$$
\hat\omega=(X^TX)^{-1}X^T\vec y
$$

当$X^TX$不满秩时，存在多组解，由算法的归纳偏好决定输出哪一组解，通常引入正则化项

广义线性模型

$$
y=g^{-1}(\omega^Tx+b)
$$

其中函数$g(.)$单调可微，称为联系函数，可以理解为将线性模型的预测值与实际标记联系起来的函数

## 对数几率回归

对于二分类任务，使用线性模型需要将线性模型的输出值与分类联系起来

使用sigmoid函数作为联系函数

$$
y=\frac{1}{1+e^{-z}}
$$

sigmoid函数将$(-\infty,+\infty)$映射到$(0,1)$之间

<img src="/assets/img/机器学习-线性模型/image-20240608230240402.png" alt="image-20240608230240402" style="zoom:50%;" />

代入线性函数有

$$
y=\frac{1}{1+e^{-(\omega^Tx+b)}}\\
\ln\frac{y}{1-y}=\omega^T+b
$$

其中$\ln\frac{y}{1-y}$​称为对数几率（logit），反映了正例的相对几率

将$y$重写为概率估计，通过极大似然估计可求解参数

$$
p(y=1\lvert x)=\frac{e^{\omega^Tx+b}}{1+e^{\omega^Tx+b}}\\
p(y=0\lvert x)=\frac{1}{1+e^{\omega^Tx+b}}
$$

## 线性判别分析

在二分类问题上，线性判别分析（LDA）通过将样本投影到一条直线上，同时使得相同类别的样本的投影点尽可能接近，而不同类别的样本的投影点尽可能远离，确定直线后，将新样本投影到该直线上，根据投影点的位置判定类别

<img src="/assets/img/机器学习-线性模型/image-20240608233543353.png" alt="image-20240608233543353" style="zoom:50%;" />

